{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-rnn-colab.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caleb-SangYoon-Lee/ML-Test/blob/master/keras_rnn_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hna0huUGwGfQ",
        "colab_type": "text"
      },
      "source": [
        "# Colab 사용법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Pj-a1wwGfS",
        "colab_type": "text"
      },
      "source": [
        "### 여기서 사용되는 예제는 Tensorflow 팀에서 제공하는 코드를 인용하였습니다.\n",
        "* https://www.tensorflow.org/guide/keras/rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NCaqjtgaEadC"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "eEDRAwTKCVfE",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "64hxAKUC2yuO"
      },
      "source": [
        "# Recurrent Neural Networks (RNN) with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dYgB0yQF3KPB"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
        "modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
        "sequence, while maintaining an internal state that encodes information about the\n",
        "timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
        "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
        "having to make difficult configuration choices.\n",
        "\n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
        "part of the `for` loop) with custom behavior, and use it with the generic\n",
        "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
        "prototype different research ideas in a flexible way with minimal code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQVvj8gyHvJ5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r5CjARa6VgYT",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LkxKP3b17LzN"
      },
      "source": [
        "## Built-in RNN layers: a simple example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "efTGph2DKzF3"
      },
      "source": [
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "1. `keras.layers.SimpleRNN`, a fully-connected RNN where the output from previous\n",
        "timestep is to be fed to next timestep.\n",
        "\n",
        "2. `keras.layers.GRU`, first proposed in\n",
        "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
        "\n",
        "3. `keras.layers.LSTM`, first proposed in\n",
        "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
        "\n",
        "In early 2015, Keras had the first reusable open-source Python implementations of LSTM\n",
        "and GRU.\n",
        "\n",
        "Here is a simple example of a `Sequential` model that processes sequences of integers,\n",
        "embeds each integer into a 64-dimensional vector, then processes the sequence of\n",
        "vectors using a `LSTM` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXPeOv1wGfo",
        "colab_type": "text"
      },
      "source": [
        "#### 아래 예제는 keras에서 (은닉)층을 어떻게 쌓는지 보여주고 있습니다.\n",
        "* 실제 사용되는 코드는 아닙니다.\n",
        "* RNN을 사용한 MNIST 예측 예제는 다음에 나옵니다.\n",
        "* keras 등의 딥러닝 프레임워크는 아래 예와 같이 거의 모든 동작을 층(layer)로 해결합니다.\n",
        " * 예를 들면 Batch Normalization 층 등을 추가하게 되면 해당 층을 통과하면서 입력데이터가 normalization 이 됩니다. 이와 같은 예로는 Embedding, Dropout 등이 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Al1mieW7pcBN",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnem4BM8L6fC"
      },
      "source": [
        "## Performance optimization and CuDNN kernels\n",
        "\n",
        "In TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN\n",
        "kernels by default when a GPU is available. With this change, the prior\n",
        "`keras.layers.CuDNNLSTM/CuDNNGRU` layers have been deprecated, and you can build your\n",
        "model without worrying about the hardware it will run on.\n",
        "\n",
        "Since the CuDNN kernel is built with certain assumptions, this means the layer **will\n",
        "not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or\n",
        "GRU layers**. E.g.:\n",
        "\n",
        "- Changing the `activation` function from `tanh` to something else.\n",
        "- Changing the `recurrent_activation` function from `sigmoid` to something else.\n",
        "- Using `recurrent_dropout` > 0.\n",
        "- Setting `unroll` to True, which forces LSTM/GRU to decompose the inner\n",
        "`tf.while_loop` into an unrolled `for` loop.\n",
        "- Setting `use_bias` to False.\n",
        "- Using masking when the input data is not strictly right padded (if the mask\n",
        "corresponds to strictly right padded data, CuDNN can still be used. This is the most\n",
        "common case).\n",
        "\n",
        "For the detailed list of constraints, please see the documentation for the\n",
        "[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM/) and\n",
        "[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU/) layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AFGAR8iHXYbe"
      },
      "source": [
        "### Using CuDNN kernels when available\n",
        "\n",
        "Let's build a simple LSTM model to demonstrate the performance difference.\n",
        "\n",
        "We'll use as input sequences the sequence of rows of MNIST digits (treating each row of\n",
        "pixels as a timestep), and we'll predict the digit's label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aWhkHxcFFbhu",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
        "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
        "input_dim = 28\n",
        "\n",
        "units = 64\n",
        "output_size = 10  # labels are from 0 to 9\n",
        "\n",
        "# Build the RNN model\n",
        "def build_model(allow_cudnn_kernel=True):\n",
        "    # CuDNN is only available at the layer level, and not at the cell level.\n",
        "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
        "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
        "    if allow_cudnn_kernel:\n",
        "        # The LSTM layer with default options uses CuDNN.\n",
        "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
        "    else:\n",
        "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
        "        lstm_layer = keras.layers.RNN(\n",
        "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
        "        )\n",
        "    model = keras.models.Sequential(\n",
        "        [\n",
        "            lstm_layer,\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dense(output_size),\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19CbiJx9jSqX"
      },
      "source": [
        "Let's load the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9vtPhdr4dsLs",
        "colab": {}
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "sample, sample_label = x_train[0], y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyTLP1qceEhX"
      },
      "source": [
        "Let's create a model instance and train it.\n",
        "\n",
        "We choose `sparse_categorical_crossentropy` as the loss function for the model. The\n",
        "output of the model has shape of `[batch_size, 10]`. The target for the model is a\n",
        "integer vector, each of the integer is in the range of 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5V23fJqvNpfv",
        "colab": {}
      },
      "source": [
        "model = build_model(allow_cudnn_kernel=True)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fZJEigasi2nU"
      },
      "source": [
        "Now, let's compare to a model that does not use the CuDNN kernel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "usiGCoLB70Ot",
        "colab": {}
      },
      "source": [
        "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
        "noncudnn_model.set_weights(model.get_weights())\n",
        "noncudnn_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "noncudnn_model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cEsHBOZx8nq1"
      },
      "source": [
        "When running on a machine with a NVIDIA GPU and CuDNN installed,\n",
        "the model built with CuDNN is much faster to train compared to the\n",
        "model that use the regular TensorFlow kernel.\n",
        "\n",
        "The same CuDNN-enabled model can also be use to run inference in a CPU-only\n",
        "environment. The `tf.device` annotation below is just forcing the device placement.\n",
        "The model will run on CPU by default if no GPU is available.\n",
        "\n",
        "You simply don't have to worry about the hardware you're running on anymore. Isn't that\n",
        "pretty cool?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mi62NdlxdWbJ",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with tf.device(\"CPU:0\"):\n",
        "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
        "    cpu_model.set_weights(model.get_weights())\n",
        "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
        "    print(\n",
        "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
        "    )\n",
        "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWtqJ9knwGgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}